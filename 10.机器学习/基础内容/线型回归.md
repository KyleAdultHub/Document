---
title: 线型回归 & 梯度下降
date: "2018-10-20 00:01:00"
categories:
- 机器学习
- 基础内容
tags:
- 机器学习
- 梯度下降
toc: true
typora-root-url: ..\..\..
---

## 1.模型表示

### **问题的概述**

> 在这里，我要根据不同房屋尺寸所售出的价格，画出我的数据集。比方说，如果你朋友的房子是1250平方尺大小，你要告诉他们这房子能卖多少钱。那么，你可以做的一件事就是构建一个模型，也许是条直线，从这个数据模型上来看，也许你可以告诉你的朋友，他能以大约220000(美元)左右的价格卖掉这个房子。这就是监督学习算法的一个例子。
>

<!-- more -->

![1539965168129](/img/1539965168129.png)

### **模型引入**

假使我们回归问题的训练集（Training Set）如下表所示：

![1539965299259](/img/1539965299259.png)

> 我们将要用来描述这个回归问题的标记如下:
> m 代表训练集中实例的数量
> x 代表特征/输入变量
> y 代表目标变量/输出变量
> (x, y) 代表训练集中的实例
> (x^i, y^i)代 表第 个观察实例
> h 代表学习算法的解决方案或函数也称为假设（hypothesis）

![1539965543172](/img/1539965543172.png)

> 这就是一个监督学习算法的工作方式，我们可以看到这里有我们的训练集里房屋价格 我们把它喂给我们的学习算法，学习算法的工作了，然后输出一个函数，通常表示为小写h表示。 h代表hypothesis(假设)， 表h示一个函数，输入是房屋尺寸大小，就像你朋友想出售的房屋，因此 h根据输入的 值来得出 y 值， y 值对应房子的价格因此， h 是一个从 x 到 y 的函数映射。
>
> 我将选择最初的使用规则代表hypothesis，因而，要解决房价预测问题，我们实际上是要将训练集“喂”给我们的学习算法，进而学习得到一个假设h，然后将我们要预测的房屋的尺寸作为输入变量输入给h，预测出该房屋的交易价格作为输出变量输出为结果。那么，对于我们的房价预测问题，我们该如何表达 h？一种可能的表达方式为：**hθ(x)=θ0+θ1∗x** ，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。

## 2.代价函数

### 什么是代价函数

![1539965938888](/img/1539965938888.png)



在线性回归中我们有一个像这样的训练集， m代表了训练样本的数量 。而我们的假设函数，也就是用来进行预测的函数，是这样的线性函数形式：**hθ(x)=θ0+θ1∗x**  。
我们通过训练获得的预测函数在预测时必然会和实际值有一些偏差，下面就给出了这部分预测偏差的定义

> **代价函数的定义**: 我们选择的参数决定了我们得到的模型相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是**建模误差（modeling error）**,下图蓝色线段变为预测和实际的误差。
>
> **平方误差代价函数**: 代价函数也被称作平方误差函数，有时也被称为平方误差代价函数。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。

![1540021643524](/img/1540021643524.png)

### 怎么优化线型回归模型

> 优化回归模型的目标: 便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得代价函数最小。
>

> 代价函数公式:
>
> ![1540029941521](/img/1540029941521.png)

### 代价函数坐标图

![1540022174651](/img/1540022174651.png)

> 可以看出在三维空间中存在一个使得J(θ_0,θ_1)代价函数最小的点， 当代价函数取得最小值的时候，我们会获得最符合训练集的预测模型。
>

## 3.梯度下降算法

### 算法介绍

> 梯度下降是一个用来求函数最小值的算法。其思想是利用迭代的思想来逼近最低点，沿着梯度的负方向更新模型权重。我们将使用梯度下降算法来求出代价函数 J(θ_0,θ_1) 的最小值。

> 梯度下降背后的思想是：开始时我们随机选择一个参数的组合（θ0, θ1, ...., θn），计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到找到一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（globalminimum），选择不同的初始参数组合，可能会找到不同的**局部最小值**。
>

![1540024168426](/img/1540024168426.png)

### 梯度下降算法公式:

**公式介绍**

repeat until convergence{ 
$$
θ_j=θ_j−α∂/∂θ_j J(θ0,θ1)　(for　j=0　and　j=1)
$$
}

![1540026766508](/img/1540026766508.png)

> 参数α: 学习率, 它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。当到达损失函数的最低点的时候，其导数为0，已经到达了局部最优点，即θ不会再发生改变

> 注意:  在梯度下降算法中，一般都会采用同步更新的方式来优化模型, 即上面公式的θj同步更新。同步更新是梯度下降中的一种常用方法。我们之后会讲到，同步更新是更自然的实现方法。当人们谈到梯度下降时，他们的意思就是同步更新。

**学习率的选择对算法的影响**

- 学习率过小的影响: 则达到收敛所需的迭代次数会非常高

- 学习率过大的影响: 每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛

**怎么确定模型是否收敛**

![1540568088758](/img/1540568088758.png)

- 梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛, 如上图在300步之后基本接近收敛。

- 也有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阀值（例如0.001）进行比较

### 梯度下降算法分类

#### **批量梯度下降**

> 在上面的线性回归场景中的梯度下降算法，有时也称为批量梯度下降。实际上，在机器学习中，通常不太会给算法起名字，但这个名字”批量梯度下降”，指的是在梯度下降的每一步中，我们都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在每一个单独的梯度下降中，我们最终都要计算这样一个东西，这个项需要对所有m个训练样本求和。

> 批量梯度下降法这个名字说明了我们需要考虑所有这一"批"训练样本，而事实上，有时也有其他类型的梯度下降法，不是这种"批量"型的，不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。

**优点：**
  （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。
  （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。

**缺点：**
  （1）当样本数目 m 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。

#### **随机梯度下降**

公式: ![1540042706219](/img/1540042706219.png)

> 随机梯度下降指在梯度下降的每一步中只用到了训练集的一个样本或者一部分样本，最后得到的可能是全局最优解，也可能是局部最优解。

**优点：**
  （1）由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。
**缺点：**
  （1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。
  （2）可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。
  （3）不易于并行实现。

## 4.梯度下降线型回归模型

### 单变量线型回归梯度下降

#### **梯度下降、线型回归算法比较**

![1540028773553](/img/1540028773553.png)

#### 单变量梯度下降公式

**代价函数计算**

![1540028837422](/img/1540028837422.png)

**参数θ的计算**

![1540028905489](/img/1540028905489.png)

### 多变量线型回归梯度下降

#### 多变量特征

现在对回归模型增加更多的特征，例如房价预测问题，增加房间数楼层等，构成一个含有多个变量的模型，模型中的特征如下图。

![1540563412916](/img/1540563412916.png)

**增添更多特征后，引入一系列新的注释**：

> - n 代表特征的数量
>
> - x^(i)^ 代表第i个训练实例，是特征矩阵的第i行，是一个向量(vector).
>
> - x~j~^(i)^ 代表特征矩阵中第i行的第j个特征，也就是第i个训练实例的第j个特征  如:  $x_2^{(2)} = 3, x_3^{(2)} = 2$
>
> - 支持多个变量的h表示为: $h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2 + ... + θ_nx_n$
>
>   - 这个公式中有个n+1参数和n个变量，为了使得公式能够简化一些，引入$x_0=1$，则公式转化为:$h_θ(x) = θ_0x_0 + θ_1x_1 + θ_2x_2 + ... + θ_nx_n$
>
> - 此时模型中的参数是一个n+1维的向量，任何一个训练实例也都是n+1维的向量，特征矩阵X的维度是m* (n+1),
>
>   因此变量的假设公式可以表示为: $h_θ(x) = θ^TX$
>

#### 多变量梯度下降公式

与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即：

$J_{(θ_0, .., θ_n)} = {1\over2m}\sum_{i=1}^m(h_θ(x^{(i)}) - y^{(i)})^2 , 其中: h_θ(x) = θ^TX = θ_0x_0 + θ_1x_1 + θ_2x_2 + ... + θ_nx_n$![1540635502085](/img/1540635502085.png)

我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。 多变量线性回归的批量梯度
**梯度下降下降公式：**

![1540566461561](/img/1540566461561.png)

**求导数后得到:**

![1540566614915](/img/1540566614915.png)

### 5.特征和多项式回归

### **特征选择**

> 有的时候的们的特征并不是很好的可以直接用来训练模型，比如房子价格预测模型的两个特征
>
> 特征: x1 房子的临街宽度， x2 房子的纵向深度
>
> 此时很明显房子的价格用 特征一和特征二的乘积，即房子的占地面积来预测更合适  $x=x_1 * x_2 = area (面积)$
>



![1540616820528](/img/1540616820528.png)



### **多项式回归**

> 很多时候，我们并没有那么幸运，正好所有的数据之间都是线性关系，能用一条直线，一个平面就描述地出来，大多数时候，我们可能需要画成曲线、曲面或者曲啥更高维度的东西
>
> 比如下图数据集，线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，用二次方程（蓝色的线）和三次方程（绿色的线）来拟合这些点，但是二次方程有个缺点，到了后面，反而会下降，这和我们的认知不符合，而三次方程不会遇到这个问题，和二次方程相加又会和上升的部分相互抵消一部分，所以最后选择了三次方程加上二次方程。
>
> 二次方模型公式：$h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2^2$      三次方模型公式: $h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2^2 + θ_3x_3^3$ 
>

![1540616493267](/img/1540616493267.png)

> 通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以把多项式回归看成是另类的多变量线性回归，我们可以令：$x_2 = x_2^2$ , $x_3 = x_3^3$ , 从而将模型转化为线型回归模型
>
> 注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要, 因为比如x的范围是1-100的话，$x^2$的范围就是1-10000，$x^3$的范围就是1-1000000。
>

## 6.特征缩放

### 为什么要特征缩放

> 在使用梯度下降的时候会遇到因为特征大小不统一，导致收敛速度过慢的问题，在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。

以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。

![1540567672941](/img/1540567672941.png)

> 解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。

### 特征缩放的两种方法

**线型归一化**

- 原理： 

- - 通过对原始数据进行变换把数据映射到(默认为[0,1])之间

- 公式

- - ![1540644955305](/img/1540644955305.png)

- 归一化的弊端

- - 使用归一化的时候，最大值和最小值非常容易受到异常点的影响， 所以这种只适合于传统精确小的数据场景

**特征标准化**

- 原理：
  - 通过对原始数据进行变换把数据变换到均值为0,方差为1的正太分布范围内

- 公式：
  - ![1540645069496](/img/1540645069496.png)
- 标准化的有点
  - 如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小, 对标准化的影响较小。

## 7.正规方程线型回归

### 正规方程算法介绍

> 对于线型回归问题经常都是在使用梯度下降算法，但是对于某些线型回归问题，正规方程是更好的解决办法。如下面的代价函数:
>

![1540619927547](/img/1540619927547.png)

![1540620250525](/img/1540620250525.png)

### 梯度下降和正规方程比较

| 梯度下降       | 正规方程 |
| -------------- | -------- |
| 需要选择学习率 | 不需要   |
|需要多次迭代|一次运算得出|
|当特征数量n特别大时能比较适用|需要计算$(X^TX)^{-1}$如果特征数量n较大则运算代价大因为矩阵逆的计算时间复杂度为$O(n^3)$，通常来说当小于10000 时还是可以接受|
|适用于各种类型的模型|只适用于线性模型，不适合逻辑回归模型等其他模型|

> 总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。

