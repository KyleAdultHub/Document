---
title: 分类算法(K近邻、朴素贝叶斯、随机森林)
date: "2018-10-21 19:50:00"
categories:
- 机器学习
- 机器学习
tags:
- 问题
- 监督学习
- 机器学习
toc: true
typora-root-url: ..\..\..
---

<html>
<body>
<a name="1550"/>

<div>
<span><div><font style="background-color: rgb(255, 250, 165);font-size: 14pt;-evernote-highlight:true;"><span style="background-color: rgb(255, 250, 165); font-size: 14pt; color: rgb(50, 135, 18); font-weight: bold;-evernote-highlight:true;">分类算法-k近邻算法</span></font></div><div><ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">k近邻算法定义</span></font></div></li><ul><li><div><span style="font-size: 9pt;">k近邻算法把每个特征当做空间的一个坐标元素</span></div></li><li><div><span style="font-size: 9pt;">如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别</span></div></li><li><div><span style="font-size: 9pt;">KNN算法最早是由Cover和Hart提出的一种分类算法</span></div></li></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">计算公式</span></font></div></li><ul><li><div><span style="font-size: 9pt;">比如说两个样本的特征值分别为，a(a1,a2,a3),  b(b1,b2,b3)</span></div></li><li><div><span style="font-size: 9pt;">两点的距离计算公式为:  <img src="/e_img/分类算法(K近邻、朴素贝叶斯、随机森林)_files/Image.png" type="image/png" data-filename="Image.png" width="246"/></span></div></li></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">KNN估计器使用方法</span></font></div></li><ul><li><div><span style="font-size: 9pt;">sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')</span></div></li><ul><li><div><span style="font-size: 9pt;">n_neighbors：int,可选（默认= 5），k_neighbors查询默认使用的邻居数</span></div></li><li><div><span style="font-size: 9pt;">algorithm：{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选用于计算最近邻居的算法：‘ball_tree’将会使用 BallTree，‘kd_tree’将使用 KDTree。‘auto’将尝试根据传递给fit方法的值来决定最合适的算法。 (不同实现方式影响效率)</span></div></li></ul><li><div><span style="font-size: 9pt;">est.fit()    训练模型</span></div></li><li><div><span style="font-size: 9pt;">est.predict()     对分类结果进行预测</span></div></li><li><div><span style="font-size: 9pt;">est.score()       对模型进行评估, 返回的是模型的准确率</span></div></li></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">KNN算法k值的影响</span></font></div></li><ul><li><div><span style="font-size: 9pt;">k值取很小：容易受噪声影响</span></div></li><li><div><span style="font-size: 9pt;">k值取很大：容易受数量的影响</span></div></li></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">KNN算法的优缺点</span></font></div></li><ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">优点</span></font></div></li><ul><li><div><span style="font-size: 9pt;">简单，易于理解，易于实现，无需估计参数</span></div></li></ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">缺点</span></font></div></li><ul><li><div><span style="font-size: 9pt;">懒惰算法，对测试样本分类时的计算量大，内存开销大</span></div></li><li><div><span style="font-size: 9pt;">必须指定K值，K值选择不当则分类精度不能保证</span></div></li></ul></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">使用场景</span></font></div></li><ul><li><div><span style="font-size: 9pt;">小数据场景，几千～几万样本，具体场景具体业务去测试</span></div></li></ul></ul><div><font style="font-size: 14pt;"><span style="font-size: 14pt; background-color: rgb(255, 250, 165); color: rgb(50, 135, 18); font-weight: bold;-evernote-highlight:true;">分类算法-朴素贝叶斯算法</span></font></div></div><div><ul><li><div><span style="font-size: 12pt; font-weight: bold;">算法公式</span></div></li><ul><li><div><span style="font-size: 9pt;"><img src="/e_img/分类算法(K近邻、朴素贝叶斯、随机森林)_files/Image [1].png" type="image/png" data-filename="Image.png" width="206"/></span></div></li><ul><li><div><span style="font-size: 9pt;">P(C|F1, F2, ...) ： 为一系列特征值对应某个分类的概率</span></div></li><li><div><span style="font-size: 9pt;">P(F1, F2, F3....|C) ： 为在C分类的条件下, 同时出现F1, F2, F3这些特征的概率</span></div></li><li><div><span style="font-size: 9pt;">P(F1, F2, F3):  为同时出现这些特征的概率</span></div></li></ul></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">常见问题</span></font></div></li><ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">概率预测为0</span></font></div></li><ul><li><div><span style="font-size: 9pt;">当某个分类下, 如果特征值出现0, 将会导致预测为该分类的概率为0，这是不合理的，如果词频列表里面有很多出现次数都为0，很可能计算结果都为零</span></div></li></ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">解决办法</span></font></div></li><ul><li><div><span style="font-size: 9pt;">拉普拉斯平滑系数</span></div></li></ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">公式</span></font></div></li><ul><li><div><span style="font-size: 9pt;"><img src="/e_img/分类算法(K近邻、朴素贝叶斯、随机森林)_files/Image [2].png" type="image/png" data-filename="Image.png" width="115"/></span></div></li><ul><li><div><span style="font-size: 9pt;">a  ： 为指定的平滑系数, 一般设置为1</span></div></li><li><div><span style="font-size: 9pt;">m:     为训练文档中统计出的特征个数</span></div></li></ul></ul></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">sklearn朴素贝叶斯实现API</span></font></div></li><ul><li><div><span style="font-size: 9pt;">sklearn.naive_bayes.MultinomialNB</span></div></li></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">使用方法</span></font></div></li><ul><li><div><span style="font-size: 9pt;">est = sklearn.naive_bayes.MultinomialNB(alpha = 1.0)      创建朴素贝叶斯估计器</span></div></li><ul><li><div><span style="font-size: 9pt;">alpha：拉普拉斯平滑系数</span></div></li></ul><li><div><span style="font-size: 9pt;">est.fit()    训练模型</span></div></li><li><div><span style="font-size: 9pt;">est.predict()     对分类结果进行预测</span></div></li><li><div><span style="font-size: 9pt;">est.score()       对模型进行评估, 返回的是模型的准确率</span></div></li></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">朴素贝叶斯分类的优缺点</span></font></div></li><ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">优点</span></font></div></li><ul><li><div><span style="font-size: 9pt;">朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率</span></div></li><li><div><span style="font-size: 9pt;">对缺失数据不太敏感，算法也比较简单，常用于文本分类</span></div></li><li><div><span style="font-size: 9pt;">分类准确度高，速度快</span></div></li></ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">缺点</span></font></div></li><ul><li><div><span style="font-size: 9pt;">由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好</span></div></li></ul></ul></ul></div><div><font style="font-size: 14pt;"><span style="font-size: 14pt; background-color: rgb(255, 250, 165); color: rgb(50, 135, 18); font-weight: bold;-evernote-highlight:true;">分类算法-决策树、随机森林</span></font></div><div><ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">决策树</span></font></div></li><ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">决策树的来源</span></font></div></li><ul><li><div><span style="font-size: 9pt;">决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-then结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法</span></div></li></ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">决策树的介绍</span></font></div></li><ul><li><div><span style="font-size: 9pt;">总结: 决策树就是根据特征对结果影响的大小进行树状排列, 对分类结果影响越大的特征, 将会更优先对分类起到决定作用 </span></div></li><li><div><span style="font-size: 9pt;"><img src="/e_img/分类算法(K近邻、朴素贝叶斯、随机森林)_files/Image [3].png" type="image/png" data-filename="Image.png" width="450"/></span></div></li></ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">决策树应用算法公式(信息熵、信息增益)</span></font></div></li><ul><li><div><font style="font-size: 9pt;"><span style="font-size: 9pt; font-weight: bold;">信息熵</span></font></div></li><ul><li><div><span style="font-size: 9pt;"><img src="/e_img/分类算法(K近邻、朴素贝叶斯、随机森林)_files/Image [4].png" type="image/png" data-filename="Image.png" width="177"/></span></div></li><li><div><span style="font-size: 9pt;">H = -(p1logp1 + p2logp2 + ... + pnlogpn)  </span></div></li><li><div><span style="font-size: 9pt;">H的专业术语称之为信息熵，单位为比特。</span></div></li></ul><li><div><font style="font-size: 9pt;"><span style="font-size: 9pt; font-weight: bold;">条件熵</span></font></div></li><ul><li><div><span style="font-size: 9pt;">已知某一特征的概率下的信息熵</span></div></li><li><div><span style="font-size: 9pt;"><img src="/e_img/分类算法(K近邻、朴素贝叶斯、随机森林)_files/Image [5].png" type="image/png" data-filename="Image.png" width="375"/></span></div></li></ul><li><div><font style="font-size: 9pt;"><span style="font-size: 9pt; font-weight: bold;">信息增益</span></font></div></li><ul><li><div><span style="font-size: 9pt;">特征A对训练数据集D的信息增益g(D,A)</span></div></li><li><div><span style="font-size: 9pt;">定义为特征数据集D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差</span></div></li><li><div><span style="font-size: 9pt;">总结： 就是条件A对分类的不确定性减少的程度</span></div></li><li><div><span style="font-size: 9pt;"><img src="/e_img/分类算法(K近邻、朴素贝叶斯、随机森林)_files/Image [6].png" type="image/png" data-filename="Image.png" width="163"/></span></div></li></ul><li><div><font style="font-size: 9pt;"><span style="font-size: 9pt; font-weight: bold;">常用的决策树算法</span></font></div></li><ul><li><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="font-size: 9pt; color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">ID3  最大信息熵增益</span></span></div></li><li><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="font-size: 9pt; color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">C4.5  </span> <span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-size: 9pt; color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">信息增益比率最大准则, 消除属性多feature值的影响</span></span></div></li><li><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="font-size: 9pt; color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">CART   回归树采用: 平方误差最小原则， 分类树采用: 基尼系数最小原则</span></span></span></div></li></ul></ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-family: Verdana, Arial, Helvetica, sans-serif; font-weight: bold;">使用方法</span></font></div></li><ul><li><div><span style="font-size: 9pt; font-family: Verdana;">sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)</span></div></li><ul><li><div><span style="font-size: 9pt; font-family: Verdana;">criterion:默认是’gini’系数，也可以选择信息增益的熵’entropy’</span></div></li><li><div><span style="font-size: 9pt; font-family: Verdana;">max_depth:树的最大深度的大小</span></div></li><li><div><span style="font-size: 9pt; font-family: Verdana;">random_state:随机数种子</span></div></li></ul></ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">将决策树结果保存本地(只有决策树有该功能, 随机森林没有)</span></font></div></li><ul><li><div><span style="font-size: 9pt;">将决策树的dot文件保存到本地</span></div></li><ul><li><div><span style="font-size: 9pt;">sklearn.tree.export_graphviz(estimator,out_file='tree.dot’,feature_names=[&quot;&quot;, ])  </span></div></li></ul><li><div><span style="font-size: 9pt;">将dot文件转化为pdf\ png</span></div></li><ul><li><div><span style="font-size: 9pt;">sudo apt-get install graphviz     安装装换工具</span></div></li><li><div><span style="font-size: 9pt;">dot -Tpng   xxx.dot  -o  xxx.png</span></div></li></ul></ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">决策树的优点及缺点</span></font></div></li><ul><li><div><span style="font-size: 9pt;">优点</span></div></li><ul><li><div><span style="font-size: 9pt;">简单的理解和解释，树木可视化。</span></div></li><li><div><span style="font-size: 9pt;">需要很少的数据准备，其他技术通常需要数据归一化</span></div></li></ul><li><div><span style="font-size: 9pt;">缺点</span></div></li><ul><li><div><span style="font-size: 9pt;">决策树学习者可以创建不能很好地推广数据的过于复杂的树，这被称为过拟合。</span></div></li></ul><li><div><span style="font-size: 9pt;">改进方法</span></div></li><ul><li><div><span style="font-size: 9pt;">减枝cart算法</span></div></li><li><div><span style="font-size: 9pt;">随机森林</span></div></li></ul></ul></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">随机森林</span></font></div></li><ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">随机森林的介绍</span></font></div></li><ul><li><div><span style="font-size: 9pt;">随机森林的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。这些预测最后结合成单预测，因此优于任何一个单分类的做出预测。</span></div></li><li><div><span style="font-size: 9pt;">随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。</span></div></li></ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">随机森林构建树的算法(用N来表示训练用例（样本）的个数，M表示特征数目)</span></font></div></li><ul><li><div><span style="font-size: 9pt;">一次随机选出一个样本，重复N次， （有可能出现重复的样本）</span></div></li><li><div><span style="font-size: 9pt;">随机去选出m个特征, m &lt;&lt;M，建立决策树</span></div></li><li><div><span style="font-size: 9pt;">采取bootstrap抽样</span></div></li></ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">抽样规则解析</span></font></div></li><ul><li><div><span style="font-size: 9pt;">为什么要随机抽样训练集？　</span></div></li><ul><li><div><span style="font-size: 9pt;">如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的</span></div></li></ul><li><div><span style="font-size: 9pt;">为什么要有放回地抽样？</span></div></li><ul><li><div><span style="font-size: 9pt;">如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是“有偏的”，都是绝对“片面的”（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决</span></div></li></ul></ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">使用方法</span></font></div></li><ul><li><div><span style="font-size: 9pt;">class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion='gini',max_depth=None, bootstrap=True, random_state=None)     创建随机森林分类器</span></div></li><ul><li><div><span style="font-size: 9pt;">n_estimators：integer，optional（default = 10） 森林里的树木数量</span></div></li><li><div><span style="font-size: 9pt;">criteria：string，可选（default =“gini”）分割特征的测量方法</span></div></li><li><div><span style="font-size: 9pt;">max_depth：integer或None，可选（默认=无）树的最大深度</span></div></li><li><div><span style="font-size: 9pt;">bootstrap：boolean，optional（default = True）是否在构建树时使用放回抽样</span></div></li><li><div><span style="font-size: 9pt;">max_depth=None, bootstrap=True, random_state=None)</span></div></li></ul></ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">随机森林的优点</span></font></div></li><ul><li><div><span style="font-size: 9pt;">在当前所有算法中，具有极好的准确率</span></div></li><li><div><span style="font-size: 9pt;">能够有效地运行在大数据集上</span></div></li><li><div><span style="font-size: 9pt;">能够处理具有高维特征的输入样本，而且不需要降维</span></div></li><li><div><span style="font-size: 9pt;">能够评估各个特征在分类问题上的重要性</span></div></li></ul></ul></ul></div><div><br/></div></span>
</div></body></html>