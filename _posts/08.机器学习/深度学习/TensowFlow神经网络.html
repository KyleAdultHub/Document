---
title: TensowFlow神经网络
date: "2018-06-04 21:00:00"
categories:
- 机器学习
- 深度学习
tags:
- tensorflow
- 深度学习
- 神经网络
toc: true
typora-root-url: ..\..\..
---


<html>
<body>
<div>
<span><div><div><font style="font-size: 14pt;"><span style="font-size: 14pt; background-color: rgb(255, 250, 165); color: rgb(50, 135, 18); font-weight: bold;-evernote-highlight:true;">神经网络介绍</span></font></div><ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">神经网络</span></font></div></li><ul><li><div><span style="font-size: 9pt;">在机器学习和认知科学领域，人工神经网络（artificial neural network，缩写ANN），简称神经网络（neural network，缩写NN）或类神经网络，是一种模仿生物神经网络的结构和功能的计算模型，用于对函数进行估计或近似。</span></div></li></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">神经网络的分类(人工神经网络)</span></font></div></li><ul><li><div><span style="font-size: 9pt; font-weight: bold;">神经网络前身</span><span style="font-size: 9pt;">:   单层感知机</span></div></li><li><div><span style="font-size: 9pt; font-weight: bold;">基础神经网络</span><span style="font-size: 9pt;">：线性神经网络，BP神经网络，Hopfield神经网络等﻿</span></div></li><li><div><span style="font-size: 9pt; font-weight: bold;">进阶神经网络</span><span style="font-size: 9pt;">：玻尔兹曼机，受限玻尔兹曼机，递归神经网络等﻿</span></div></li><li><div><span style="font-size: 9pt; font-weight: bold;">深度神经网络</span><span style="font-size: 9pt;">：深度置信网络，卷积神经网络(CNN)，循环神经网络，LSTM网络等</span></div></li></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">神经网络的特点</span></font></div></li><ul><li><div><span style="font-size: 9pt;">输入向量的特征数量和输入神经元的个数相同</span></div></li><li><div><span style="font-size: 9pt;">输出层的神经元数量和种类数量相同</span></div></li><li><div><span style="font-size: 9pt;">每个连接都有个权重值</span></div></li><li><div><span style="font-size: 9pt;">同一层神经元之间没有连接</span></div></li><li><div><span style="font-size: 9pt;"> 由输入层，隐层，输出层组成</span></div></li><li><div><span style="font-size: 9pt;">第N层与第N-1层的所有神经元连接，也叫全连接</span></div></li></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">简单的神经网络模型</span></font></div></li><ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">神经网络模型示例</span></font></div></li><ul><li><div><span style="font-size: 9pt;">输入层、隐层、输出层  （隐层和输出层统称为全连接层）</span></div></li></ul><li><div><span style="font-size: 9pt;"><img src="/e_img/TensowFlow神经网络_files/Image.png" type="image/png" data-filename="Image.png" width="199"/><img src="/e_img/TensowFlow神经网络_files/Image [1].png" type="image/png" data-filename="Image.png" width="370"/></span></div></li><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">神经元模型示例</span></font></div></li><ul><li><div><span style="font-size: 9pt;">输入向量的特征数量和输入神经元的个数相同</span></div></li></ul><li><div><span style="font-size: 9pt;"><img src="/e_img/TensowFlow神经网络_files/Image [2].png" type="image/png" data-filename="Image.png" width="243"/>中间层三个神经元的示例</span></div></li></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">神经网络的组成</span></font></div></li><ul><li><div><span style="font-size: 9pt;">结构： 组成神经网络的神经元</span></div></li><li><div><span style="font-size: 9pt;">激励函数：隐层和输出层节点的输入和输出之间具有函数关系，这个函数称为激励函数</span></div></li><li><div><span style="font-size: 9pt;">学习规则：学习规则制定了网络中的权重如何随着时间的推进而调整(反向传播算法)</span></div></li></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">TensowFlow提供神经网络功能的主要模块</span></font></div></li><ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">tf.nn：</span></font></div></li><ul><li><div><span style="font-size: 9pt;">神经网络相关操作的支持，包括卷积操作（conv）、池化操作（pooling）、归一化、loss、分类操作、embedding、RNN、Evaluation</span></div></li></ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">tf.layers:</span></font></div></li><ul><li><div><span style="font-size: 9pt;">主要提供的高层的神经网络，主要和卷积相关的，对tf.nn的进一步封装</span></div></li></ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">tf.contrib:</span></font></div></li><ul><li><div><span style="font-size: 9pt;">tf.contrib.layers提供够将计算图中的  网络层、正则化、摘要操作、是构建计算图的高级操作</span></div></li></ul></ul></ul><div><font color="#328712" style="font-size: 14pt;"><span style="font-size: 14pt; background-color: rgb(255, 250, 165); color: rgb(50, 135, 18); font-weight: bold;-evernote-highlight:true;">神经网络前身--感知机</span></font></div><ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">什么是感知机</span></font></div></li><ul><li><div><span style="font-size: 9pt;">有n个输入数据，通过权重与各数据之间进行加权求和，求和后的结果经过激活函数结果(0/1)，得出输出</span></div></li><li><div><span style="font-size: 9pt;">很容易解决与、或问题</span></div></li><li><div><span style="font-size: 9pt;">通过多个感知机，可以将结果分类的区域划分的越来越准确</span></div></li></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">用模型表示</span></font></div></li><ul><li><div><span style="font-size: 9pt;"><img src="/e_img/TensowFlow神经网络_files/Image [3].png" type="image/png" data-filename="Image.png" width="342"/></span></div></li></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">感知机与逻辑回归的联系和区别</span></font></div></li><ul><li><div><span style="font-size: 9pt;">联系: 都可以用来做分类的学习</span></div></li><li><div><span style="font-size: 9pt;">区别：激活函数不一样，感知机使用的额是sign函数,输出值是0/1，只能判断简单的分类问题, 逻辑回归使用的是sigmid函数, 结果为0-1, 可以预测为某一个分类的准确概率, 更为广泛使用</span></div></li></ul></ul><div><font style="font-size: 14pt;"><span style="font-size: 14pt; background-color: rgb(255, 250, 165); color: rgb(50, 135, 18); font-weight: bold;-evernote-highlight:true;">浅层(单一隐层)神经网络</span></font></div><ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">概念</span></font></div></li><ul><li><div><span style="font-size: 9pt;">只有一层隐藏层的神经网络</span></div></li></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">目标值处理</span></font></div></li><ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">one-hot编码</span></font></div></li><ul><li><div><span style="font-size: 9pt;"><img src="/e_img/TensowFlow神经网络_files/Image [4].png" type="image/png" data-filename="Image.png" width="331"/></span></div></li></ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">one-hot api介绍</span></font></div></li><ul><li><div><span style="font-size: 9pt;">作用:  将一组目标集转化为one-hot编码</span></div></li><li><div><span style="font-size: 9pt;">tf.one_hot(indices, depth, on_value=None, off_value=None, axis=None, dtype=None, name=None)</span></div></li><ul><li><div><span style="font-size: 9pt;">indices: 数据集标签, 目标值集合</span></div></li><li><div><span style="font-size: 9pt;">depth: 张量的深度,  即类别数</span></div></li></ul></ul></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">SoftMax回归函数</span></font></div></li><ul><li><div><font style="font-size: 9pt;"><span style="font-size: 9pt; font-weight: bold;">作用: </span></font></div></li><ul><li><div><span style="font-size: 9pt;">将一组结果数据转化为[0, 1] 两个之间的数, 为1的数代表预测结果为该分类</span></div></li></ul><li><div><font style="font-size: 9pt;"><span style="font-size: 9pt; font-weight: bold;">公式</span></font></div></li><ul><li><div><span style="font-size: 9pt;"> <img src="/e_img/TensowFlow神经网络_files/Image [5].png" type="image/png" data-filename="Image.png" width="98"/></span></div></li></ul><li><div><font style="font-size: 9pt;"><span style="font-size: 9pt; font-weight: bold;">使用位置</span></font></div></li><ul><li><div><span style="font-size: 9pt;">在特征值经过加权和偏置计算后，经过softmax函数, 得到最终预测的概率结果</span></div></li></ul><li><div><font style="font-size: 9pt;"><span style="font-size: 9pt; font-weight: bold;">图示</span></font></div></li><ul><li><div><span style="font-size: 9pt;"><img src="/e_img/TensowFlow神经网络_files/Image [6].png" type="image/png" data-filename="Image.png" width="376"/></span></div></li><li><div><span style="font-size: 9pt;"><img src="/e_img/TensowFlow神经网络_files/Image [7].png" type="image/png" data-filename="Image.png" width="395"/></span></div></li></ul></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">特征的加权计算</span></font></div></li><ul><li><div><font style="font-size: 9pt;"><span style="font-size: 9pt; font-weight: bold;">tensorflow加权计算api</span></font></div></li><ul><li><div><span style="font-size: 9pt;">tf.matmul(a, b,name=None)</span></div></li><ul><li><div><span style="font-size: 9pt;">a: 特征值矩阵</span></div></li><li><div><span style="font-size: 9pt;">b: 加权矩阵</span></div></li></ul></ul></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">损失计算</span></font></div></li><ul><li><div><font style="font-size: 9pt;"><span style="font-size: 9pt; font-weight: bold;"> 损失计算-交叉熵损失公式</span></font></div></li><ul><li><div><span style="font-size: 9pt;"><img src="/e_img/TensowFlow神经网络_files/Image [8].png" type="image/png" data-filename="Image.png" width="202"/></span></div></li><li><div><span style="font-size: 9pt;"><img src="/e_img/TensowFlow神经网络_files/Image [9].png" type="image/png" data-filename="Image.png" width="224"/></span></div></li></ul><li><div><font style="font-size: 9pt;"><span style="font-size: 9pt; font-weight: bold;">损失计算api</span></font></div></li><ul><li><div><span style="font-size: 9pt;">s = tf.nn.softmax_cross_entropy_with_logits(labels=None, logits=None,name=None)     计算交叉熵损失</span></div></li><ul><li><div><span style="font-size: 9pt;">labels:标签值（真实值）</span></div></li><li><div><span style="font-size: 9pt;">logits：样本加权之后的值</span></div></li><li><div><span style="font-size: 9pt;">return:返回损失值列表</span></div></li></ul></ul><li><div><font style="font-size: 9pt;"><span style="font-size: 9pt; font-weight: bold;">损失均值计算api</span></font></div></li><ul><li><div><span style="font-size: 9pt;">tf.reduce_mean(s)   获取损失的平均值</span></div></li></ul></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">损失的优化--梯度下降</span></font></div></li><ul><li><div><span style="font-size: 9pt;"> tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</span></div></li><ul><li><div><span style="font-size: 9pt;">learning_rate:  学习率，一般为</span></div></li><li><div><span style="font-size: 9pt;">minimize(loss):  最小化损失</span></div></li><li><div><span style="font-size: 9pt;">return:  梯度下降op</span></div></li></ul></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">模型准确性计算</span></font></div></li><ul><li><div><span style="font-size: 9pt;">equal_list = tf.equal(tf.argmax(y, 1), tf.argmax(y_label, 1))   <span style="font-size: 9pt;">返回每一行的最大值的索引</span></span></div></li><li><div><span style="font-size: 9pt;"><span style="font-size: 9pt;">equal_list = tf.equal(tf.argmax(y, 0), tf.argmax(y_label, 0))   </span><span style="font-size: 9pt;">返回每一列的最大值的索引</span></span></div></li><li><div><span style="font-size: 9pt;"><span style="font-size: 9pt;">equal_list = tf.equal(tf.argmax(y, 2), tf.argmax(y_label, 2))   </span><span style="font-size: 9pt;">返回每行每列的最大值</span></span></div></li><li><div><span style="font-size: 9pt;">accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))</span></div></li><ul><li><div><span style="font-size: 9pt;">y: 预测结果的集合</span></div></li><li><div><span style="font-size: 9pt;">x: 目标集合</span></div></li></ul></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">神经网络实现流程</span></font></div></li><ul><li><div><span style="font-size: 9pt;">1、准备数据</span></div></li><li><div><span style="font-size: 9pt;">2、全连接结果计算</span></div></li><li><div><span style="font-size: 9pt;">3、损失优化</span></div></li><li><div><span style="font-size: 9pt;">4、模型评估（计算准确性）</span></div></li></ul></ul><div><font style="font-size: 14pt;"><span style="font-size: 14pt; background-color: rgb(255, 250, 165); color: rgb(50, 135, 18); font-weight: bold;-evernote-highlight:true;">深层神经网络(卷积神经网络介绍)</span></font></div><ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">概念</span></font></div></li><ul><li><div><span style="font-size: 9pt;">深度学习网络与更常见的单一隐藏层神经网络的区别在于深度，深度学习网络中，每一个节点层在前一层输出的基础上学习识别一组特定的特征。随着神经网络深度增加，节点所能识别的特征也就越来越复杂</span></div></li></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">为什么引入深层神经网络</span></font></div></li><ul><li><div><span style="font-size: 9pt;">全连接神经网络具有一定的局限性</span></div></li><li><div><span style="font-size: 9pt;">当数据的特征特别多的时候, 每一个特征值都会有一个权重, 这时每一个神经元都需要很多个权重值</span></div></li><li><div><span style="font-size: 9pt;">这样就会很影响资源， 造成资源的浪费， 影响训练</span></div></li><li><div><span style="font-size: 9pt;">增加隐藏层数, 可以每层逐渐的减少所需权重的数量</span></div></li></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">什么是卷积神经网络</span></font></div></li><ul><li><div><span style="font-size: 9pt;">卷积神经网络，是一种前馈神经网络，人工神经元可以响应周围单元，可以进行大型图像处理。卷积神经网络包括卷积层和池化层。</span></div></li></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">卷积神经网络的发展历史</span></font></div></li><ul><li><div><span style="font-size: 9pt;"><img src="/e_img/TensowFlow神经网络_files/Image [10].png" type="image/png" data-filename="Image.png" width="543"/></span></div></li></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">卷积神经网络的特点</span></font></div></li><ul><li><div><span style="font-size: 9pt;">神经网络(neural networks)的基本组成包括输入层、隐藏层、输出层。</span></div></li><li><div><span style="font-size: 9pt;">而卷积神经网络的特点在于隐藏层分为卷积层和池化层(pooling layer，又叫下采样层)</span></div></li></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">卷积神经网络的结构分析</span></font></div></li><ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">训练结构图</span></font></div></li><ul><li><div><span style="font-size: 9pt;"><img src="/e_img/TensowFlow神经网络_files/Image [11].png" type="image/png" data-filename="Image.png" width="568"/></span></div></li></ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">卷积层(CONV): </span></font></div></li><ul><li><div><span style="font-size: 9pt;"><span style="font-size: 9pt; font-weight: bold;">卷积层介绍</span></span></div></li><ul><li><div><span style="font-size: 9pt;">通过Filter(过滤器)在原始图像上平移来提取并转化特征, 使转化后的特征可以提供给池化层进行数据的池化</span></div></li></ul><li><div><span style="font-size: 12px;"><span style="font-size: 12px; font-weight: bold;">卷积层过滤器参数介绍</span></span></div></li><ul><li><div><span style="font-size: 9pt;">个数</span></div></li><li><div><span style="font-size: 9pt;">大小(1*1,3*3,5*5)</span></div></li><li><div><span style="font-size: 9pt;">步长</span></div></li><li><div><span style="font-size: 9pt;">零填充</span></div></li></ul><li><div><span style="font-size: 12px;"><span style="font-size: 12px; font-weight: bold;">卷积层输出的参数介绍</span></span></div></li><ul><li><div><span style="font-size: 9pt;">深度由过滤器个数决定</span></div></li><li><div><span style="font-size: 9pt;">输出长度和宽度：由filter尺寸和步长决定</span></div></li></ul></ul><li><div><span style="font-size: 10pt; font-weight: bold;">RELU激活函数</span></div></li><ul><li><div><span style="font-size: 9pt; font-weight: bold;">使用Relu函数的优点</span></div></li><ul><li><div><span style="font-size: 9pt;">第一，采用sigmoid等函数，反向传播求误差梯度时，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多</span></div></li><li><div><span style="font-size: 9pt;">第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（求不出权重和偏置）</span></div></li></ul><li><div><span style="font-size: 9pt; font-weight: bold;">函数图形</span></div></li><li><div><img src="/e_img/TensowFlow神经网络_files/Image [12].png" type="image/png" data-filename="Image.png" style="font-weight: bold;" width="346"/></div></li></ul></ul></ul><div><br/></div><ul><ul><li><div><span style="font-size: 9pt; font-weight: bold;">激活函数使用api</span></div></li><ul><li><div><span style="font-size: 9pt;">tf.nn.relu(features, name=None)</span></div></li><ul><li><div><span style="font-size: 9pt;">features:卷积后加上偏置的结果</span></div></li><li><div><span style="font-size: 9pt;">return:结果</span></div></li></ul></ul></ul><li><div><font style="font-size: 10pt;"><span style="font-size: 10pt; font-weight: bold;">池化层(POOL):</span></font></div></li><ul><li><div><span style="font-size: 9pt; font-weight: bold;">池化层介绍</span></div></li><ul><li><div><span style="font-size: 9pt;">通过特征后稀疏参数来减少学习的参数，降低网络的复杂度，（最大池化和平均池化）</span></div></li><li><div><span style="font-size: 9pt;">Pooling层主要的作用是特征提取，通过去掉Feature Map中不重要的样本，进一步减少参数数量。</span></div></li><li><div><span style="font-size: 9pt;">Pooling的方法很多，最常用的是Max Pooling。2*2  2步长</span></div></li></ul><li><div><span style="font-size: 9pt; font-weight: bold;">数据的池化过程图例</span></div></li><ul><li><div><img src="/e_img/TensowFlow神经网络_files/Image [13].png" type="image/png" data-filename="Image.png" width="473"/></div></li></ul><li><div><span style="font-size: 9pt; font-weight: bold;">池化使用api</span></div></li><ul><li><div><span style="font-size: 9pt;">tf.nn.max_pool(value, ksize=, strides=, padding=,name=None)   输入上执行最大池数</span></div></li><ul><li><div><span style="font-size: 9pt;">value:4-D Tensor形状[batch, height, width, channels]</span></div></li><li><div><span style="font-size: 9pt;">ksize:池化窗口大小，[1, ksize, ksize, 1]</span></div></li><li><div><span style="font-size: 9pt;">strides:步长大小，[1,strides,strides,1]</span></div></li><li><div><span style="font-size: 9pt;">padding:“SAME”, “VALID”，使用的填充算法的类型，使用“SAME”</span></div></li></ul></ul></ul><li><div><span style="font-size: 10pt; font-weight: bold;">全连接层(FC):</span></div></li><ul><li><div><span style="font-size: 12px;"><span style="font-size: 12px; font-weight: bold;">全连接层在卷积神经网络中介绍</span></span></div></li><ul><li><div><span style="font-size: 9pt;">前面的卷积和池化相当于做特征工程，后面的全连接相当于做特征加权。最后的全连接层在整个卷积神经网络中起到“分类器”的作用。</span></div></li></ul></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">卷积神经网络相关参数计算</span></font></div></li><ul><li><div><font style="font-size: 9pt;"><span style="font-size: 9pt; font-weight: bold;">输入值的体积</span></font></div></li><ul><li><div><span style="font-size: 9pt;">H1 * W1 * D1</span></div></li></ul><li><div><font style="font-size: 9pt;"><span style="font-size: 9pt; font-weight: bold;">卷积层过滤器四个超参数</span></font></div></li><ul><li><div><span style="font-size: 9pt;">Filter数量K</span></div></li><li><div><span style="font-size: 9pt;">Filter大小K</span></div></li><li><div><span style="font-size: 9pt;">步长S</span></div></li><li><div><span style="font-size: 9pt;">零填充大小P</span></div></li></ul><li><div><font style="font-size: 9pt;"><span style="font-size: 9pt; font-weight: bold;">卷积层过滤器输出体积大小</span></font></div></li><ul><li><div><span style="font-size: 9pt;">H2 = (H1-F+2P)/S + 1</span></div></li><li><div><span style="font-size: 9pt;">W2 = (W1-F+2P)/S +1</span></div></li><li><div><span style="font-size: 9pt;">D2 = K</span></div></li></ul></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">数据特征的变化过程</span></font></div></li><ul><li><div><span style="font-size: 9pt; font-weight: bold;">数据在经过多个卷积层后的变化</span></div></li><ul><li><div><span style="font-size: 9pt;">宽度和高度在缩小, 深度不断的增加(深度和卷积层的过滤器数量相关)</span></div></li><li><div><span style="font-size: 9pt;"><img src="/e_img/TensowFlow神经网络_files/Image [14].png" type="image/png" data-filename="Image.png" width="365"/></span></div></li></ul><li><div><font style="font-size: 9pt;"><span style="font-size: 9pt; font-weight: bold;">单个卷积层(2个Filter)的过程示例</span></font></div></li><ul><li><div><span style="font-size: 9pt;"><img src="/e_img/TensowFlow神经网络_files/Image [15].png" type="image/png" data-filename="Image.png" width="500"/></span></div></li></ul></ul><li><div><font style="font-size: 12pt;"><span style="font-size: 12pt; font-weight: bold;">卷积层的零填充</span></font></div></li><ul><li><div><font style="font-size: 9pt;"><span style="font-size: 9pt; font-weight: bold;">定义</span></font></div></li><ul><li><div><span style="font-size: 9pt;">卷积核在提取特征映射时的动作称之为padding（零填充)</span></div></li><li><div><span style="font-size: 9pt;">由于移动步长不一定能整出整张图的像素宽度。其中有两种方式，SAME和VALID</span></div></li></ul><li><div><font style="font-size: 9pt;"><span style="font-size: 9pt; font-weight: bold;">两种零填充的方式</span></font></div></li><ul><li><div><span style="font-size: 9pt;">1. SAME：越过边缘取样，取样的面积和输入图像的像素宽度一致。</span></div></li><li><div><span style="font-size: 9pt;">2. VALID：不越过边缘取样，取样的面积小于输入人的图像的像素宽度</span></div></li></ul><li><div><font style="font-size: 9pt;"><span style="font-size: 9pt; font-weight: bold;">卷积网络卷积操作零填充api介绍</span></font></div></li><ul><li><div><span style="font-size: 9pt;">tf.nn.conv2d(input, filter, strides=, padding=, name=None)    卷计划操作</span></div></li><ul><li><div><span style="font-size: 9pt;">input：给定的输入张量，具有[batch,heigth,width, channel]，类型为float32,64</span></div></li><li><div><span style="font-size: 9pt;">filter：指定过滤器的大小，[filter_height, filter_width,in_channels, out_channels]</span></div></li><li><div><span style="font-size: 9pt;">strides：strides = [1, stride, stride, 1],步长</span></div></li><li><div><span style="font-size: 9pt;">padding：“SAME”, “VALID”，使用的填充算法的类型，使用“SAME”。其中”VALID”表示滑动超出部分舍弃，SAME”表示填充，使得变化后height,width一样大</span></div></li></ul></ul></ul></ul><div><span style="background-color: rgb(255, 250, 165); font-size: 12pt; color: rgb(50, 135, 18); font-weight: bold;-evernote-highlight:true;">神经网络总结</span></div><ul><li><div><span style="font-size: 9pt;">输入层的矩阵的特征数量和特征的数量(样本的列数)相等</span></div></li><li><div><span style="font-size: 9pt;">输出层的矩阵的列数与分类数量相等</span></div></li><li><div><span style="font-size: 9pt;">经过矩阵变换的矩阵: 行数=左矩阵的行， 列数= 右矩阵的列</span></div></li><li><div><span style="font-size: 9pt;">每个矩阵代表全连接层的一层隐层</span></div></li><li><div><span style="font-size: 9pt;">矩阵中的每一列代表一个神经元</span></div></li><li><div><span style="font-size: 9pt;">一个全连接层和一有多个隐层，即多个矩阵</span></div></li><li><div><span style="font-size: 9pt;">偏置的列数应该等于矩阵的行数</span></div></li></ul></div><div><br/></div></span>
</div></body></html>