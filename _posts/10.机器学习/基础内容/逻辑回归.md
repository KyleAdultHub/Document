---
title: 逻辑回归 & 分类问题
date: "2018-10-31 22:16:00"
categories:
- 机器学习
- 基础内容
tags:
- 机器学习
- 逻辑回归
- 分类问题
toc: true
typora-root-url: ..\..\..
---

## 一、逻辑回归介绍

### 1.1分类问题

在分类问题中，如果想要预测的变量y是离散的值，那么可以只用一种叫做逻辑回归(logistic regression)的算法,这是在目前最流行的一种学习算法之一。

在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。分类问题的例子有：判断一封电子邮件是否是垃圾邮件；判断一次金融交易是否是欺诈；之前我们也谈到了肿瘤分类问题的例子，区别一个肿瘤是恶性的还是良性的。  

<!-- more -->

![1540997286168](/img/1540997286168.png)

我们将因变量(dependent variable)可能属于的两个类分别称为负向类（negative class）和正向类（positive class），则因变量y∈0,1 ，其中 0 表示负向类，1 表示正向类。

![1540997531192](/img/1540997531192.png)

#### 线型回归不适合分类问题

如果我们要用线性回归算法来解决一个分类问题，对于分类， y 取值为 0 或者1，但如果你使用的是线性回归，那么假设函数的输出值可能远大于 1，或者远小于0，即使所有训练样本的标签 y 都等于 0 或 1。尽管我们知道标签应该取值0 或者1，但是如果算法得到的值远大于1或者远小于0的话，就会感觉很奇怪。所以我们在接下来的要研究的算法就叫做逻辑回归算法，这个算法的性质是：它的输出值永远在0到 1 之间。

#### 逻辑回归算法

逻辑回归算法是分类算法，我们将它作为分类算法使用。有时候可能因为这个算法的名字中出现了“回归”使你感到困惑，但逻辑回归算法实际上是一种分类算法，它适用于标签 y 取值离散的情况，如：1 0 0 1。

### 1.2逻辑回归表达式

在分类问题中，要用什么样的函数来表示我们的假设。在解决分类问题的时候，我们希望分类器的输出值在0和1之间，因此，我们希望想出一个满足某个性质的假设函数，这个性质是它的预测值要在0和1之间。

#### 为什么线型回归不适合分类问题?

回顾在一开始提到的乳腺癌分类问题，我们可以用线性回归的方法求出适合数据的一条直线：

![1541078937881](/img/1541078937881.png)

根据线性回归模型我们只能预测连续的值，然而对于分类问题，我们需要输出0或1，我们可以预测：

当 $h_θ(x) >= 0.5$ 时, 预测y= 1;   当$h_θ(x) < 0.5$ 时，预测y = 0

对于上图所示的数据，这样的一个线性模型似乎能很好地完成分类任务。但是， 假使我们又观测到一个非常大尺寸的恶性肿瘤，将其作为实例加入到我们的训练集中来，这将使得我们获得一条新的直线。

![1541079100064](/img/1541079100064.png)

这时，再使用0.5作为阀值来预测肿瘤是良性还是恶性便不合适了， 所以使用线型回归模型，很难确定断定一个问题分类的边界。可以看出，线性回归模型，因为其预测的值可以超越[0,1]的范围，并不适合解决分类这样的问题。

#### 逻辑回归模型

我们引入一个新的模型，逻辑回归，该模型的输出变量范围始终在0和1之间。 逻辑回归模型的假设是：

$h_θ(x) = g(θ^TX)$      函数g的表达式为:  $g(z) = {1\over1+e^{-z}}$

>  其中: X 代表特征向量, g代表逻辑函数（logistic function)是一个常用的逻辑函数为S形函数（Sigmoid function）

g(z) 的函数图像为:

![1541080097808](/img/1541080097808.png)

> 对逻辑回归模型理解
>
> $h_θ(x)$ 的作用是，对于给定的输入变量，根据选择的参数计算出输出的变量=1 的可能性, 即$h_θ(x) =P(y=1|x; θ)$ 
>
> 例如, 如果对于给定的x， 通过已经确定的参数计算出$h_θ(x) = 0.7$ ， 则表示有70%的几率y为正向类，响应地y为负向类的几率为1-0.7=0.3
>
> g(z) （Sigmoid function）函数的作用就是将$θ^TX$的结果映射到0-1之间，

### 1.3决策边界

#### 对决策边界的理解

> 决策边界(decision boundary)，能够更好地帮助我们理解逻辑回归的假设函数中计算什么

![1541080767309](/img/1541080767309.png)

在逻辑回归中， 我们预测到: 

当$h_θ(x) >= 0.5$ 时， 预测 y= 1;     当 $h_θ(x) >= 0.5$ 时，预测 y  = 0；

根据上面绘制的S形函数图像，我们知道当

z = 0 时, g(z) = 0.5

z > 时, g(z) > 0.5

z < 0 时, g(z) < 0.5

又 $z = θ^Tx$, 即: $θ^TX >= 0$ 时, 预测 y = 1 , $θ^TX < 0$ 时， 预测 y = 0

现在假设我们有一个模型: 

![1541081223807](/img/1541081223807.png)

并且参数θ是向量[-3 1 1]. 则当 -3 + x1 + x2 >= 0, 即x1 + x2 >= 3时， 模型将预测 y = 1. 我们可以绘制直线x1 + x2 = 3, 这条线便是我们模型的分界线，将预测为1的区域和预测为 0的区域分隔开。

![1541081375902](/img/1541081375902.png)

#### 复杂形状的决策边界

假使我们的数据呈现这样的分布情况，怎样的模型才能适合呢？

![1541081567959](/img/1541081567959.png)

因为需要用曲线才能分隔 y = 0 的区域和 y = 1 的区域，我们需要二次方特征：

所以按照假设函数应该趋近， $h_θ(x) = g(θ_0 + θ_1x_1 + θ_2x_2 + θ_1x_1^2 + θ_2x_2^2 )$  θ最终结果趋近 [-1 0 0 1 1], 我们最终得到的判定边界恰好是圆点在原点且半径为1的圆形 , 综上我们可以用复杂的模型来适应飞象复杂形状的判定边界

### 1.4 逻辑回归代价函数和梯度下降

#### 逻辑回归、线性回归代价函数区别

对于线性回归模型，我们定义的代价函数是所有模型误差的平方和和。理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于，当我们将 $h_θ(x) = g(θ^TX)$  带入到这样定义了的代价函数中时，我们得到的代价函数将是一个非凸函数（non-convexfunction）如下图，这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。

![1541082558471](/img/1541082558471.png)

线型回归代价函数为: $j_{(θ)} = {1\over m} \sum_{i=1}^m{1\over2}(h_θ(x^{(i)})-y^{(i)})^2$

重新定义逻辑回归代价函数，使代价函数在逻辑回归算法中为凸函数, 过程如下: 

![1541083131135](/img/1541083131135.png)

> 根据推倒可以证明, 我们推倒的的代价值函数会给我们一个凸优化问题。代价函数 J(θ) 会是一个凸函数，并且没有局部最优值。

**python代码实现**

```python
import numpy as np
def cost(theta, X, y):
   theta = np.matrix(theta)
   X = np.matrix(X)
   y = np.matrix(y)
   first = np.multiply(‐y, np.log(sigmoid(X* theta.T)))
   second = np.multiply((1 ‐ y), np.log(1 ‐ sigmoid(X* theta.T)))
   return np.sum(first ‐ second) / (len(X))
```

#### 梯度下降算法推倒

在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为：

Repeat {  $θ_j : =θ_j  −  α{∂\over∂θ_j }J(θ)$         (simultaneously update all ) }

求导后得到: 

Repeat { $θ_j : =θ_j  −  α{1\over m}\sum_{i=1}^m(h_θ(x^{(i)}) - y^{(i)})x_j^{(i)}$    (simultaneously update all ) }

![1541084166476](/img/1541084166476.png)

> 注：虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样，但是这里的 $h_θ(x) = g(θ^TX)$ 与线性回归中不同，所以实际上是不一样的。另外，在运行梯度下降算法之前，进行特征缩放依旧是非常必要的。

#### 比梯度下降算法更好的选择

一些梯度下降算法之外的选择： 除了梯度下降算法以外，还有一些常被用来令代价函数最小的算法，这些算法更加复杂和优越，而且通常不需要人工选择学习率，通常比梯度下降算法要更加快速。

这些算法有: 1.共轭梯度（Conjugate Gradient），2.局部优化法(Broyden etcher goldfarb shann,BFGS) ,  3.有限内存局部优化法(LBFGS) , 4. fminunc  ( 由matlab和octave提供的算法 )

**fminunc**是 matlab和octave 中都带的一个最小值优化函数，使用时我们需要提供代价函数和每个参数的求导，下面是 octave 中使用 **fminunc 函数的代码示例：**

1. 定义函数 返回内容为[ 代价函数， 代价函数分别对于每一个θ的导数 ]
2. 在终端定义options， 用来存储相关参数的options数据结构； GradObj  on  代表打开梯度目标, MaxIter 100 表示最大下降步数为100
3. 在终端给出θ的初始值
4. 调用fminunc算法， 需要传入我们定义的函数名，并加上@符号和 θ初始值，和options数据结构
5. 返回值optTheta 最终的θ值，functionVal为最终损失函数的值，exitFlag为收敛标志 1表示已经收敛

```
function [jVal, gradient] = costFunction(theta)
	 jVal = [...code to compute J(theta)...];
	 gradient = [...code to compute derivative of J(theta)...];

end

options = optimset('GradObj', 'on', 'MaxIter', '100');

initialTheta = zeros(2,1);

[optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);
```

1.5简化代价函数和梯度下降算法