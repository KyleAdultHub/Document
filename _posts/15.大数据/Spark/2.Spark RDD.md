---
window
title: Spark RDD
date: "2020-03-31 16:00:00"
categories:
- 大数据
- Spark
tags:
- 大数据
toc: true
typora-root-url: ..\..\..
---

## RDD 介绍

### 什么是RDD

RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。

### RDD的属性

1）一组分区（Partition），即数据集的基本组成单位。对于RDD来说，每个分区都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分区个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。

2）一个计算每个分区的函数。Spark中RDD的计算是以分区为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。

3）RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。

4）一个Partitioner，即RDD的分区函数。当前Spark中实现了两种类型的分区函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分区数量，也决定了parent RDD Shuffle输出时的分区数量。

5）一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。

### 创建RDD的方式

1）由一个已经存在的Scala集合创建。

val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8))

2）由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等

val rdd2 = sc.textFile("hdfs://node1.itcast.cn:9000/words.txt")

### RDD 编程 API

RDD中的所有转换（Transform）都是延迟加载的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果的计算(Action)给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率地运行。

### Transformation

**常用transformation**

| **转换**                                                 | **含义**                                                     | 使用                                                         |
| -------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **map**(func)                                            | 返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成 | sc.parallelize(List(5,6,4,7,3,8,2,9,1,10)).map(\_\*2)        |
| **filter**(func)                                         | 返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成 | rdd2.filter(\_>10)                                           |
| **flatMap**(func)                                        | 返回一个新的RDD，RDD中元素经过func函数计算后，并将所返回的内容打散平铺 | sc.textFile("/root/words.txt").flatMap(x=>x.split(" "))      |
| **mapPartitions**(func)                                  | 类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] => Iterator[U] | val rdd4 = rdd3.partitionBy(hostParitioner).mapPartitions(it => {   it.toList.sortBy(\_.\_2.\_2).reverse.take(2).iterator }) rdd4.saveAsTextFile("c://out4") |
| **mapPartitionsWithIndex**(func)                         | 类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) => Iterator[U] | val func = (index: Int, iter: Iterator[(Int)]) => { iter.toList.map(x => "[partID:" +  index + ", val: " + x + "]").iterator};val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)rdd1.mapPartitionsWithIndex(func).collect |
| **sample**(withReplacement, fraction, seed)              | 根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器种子 |                                                              |
| **union**(otherDataset)                                  | 对源RDD和参数RDD求并集后返回一个新的RDD，要求列数要一样，类型可以不同 |                                                              |
| **intersection**(otherDataset)                           | 对源RDD和参数RDD求交集后返回一个新的RDD                      | val rdd9 = rdd6.intersection(rdd7)                           |
| **distinct**([numTasks]))                                | 对源RDD进行去重后返回一个新的RDD                             | val rdd6 = sc.parallelize(List(5,6,4,7))val rdd7 = sc.parallelize(List(1,2,3,4)); val rdd8 = rdd6.union(rdd7)； rdd8.distinct.sortBy(x=>x).collect |
| **groupByKey**([numTasks])                               | 在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD        | val rdd1 = sc.parallelize(List(("tom", 1), ("jerry", 2), ("kitty", 3))) ;val rdd2 = sc.parallelize(List(("jerry", 9), ("tom", 8), ("shuke", 7))); val rdd3 = rdd1 union rdd2; rdd3.groupByKey.map(x=>(x.\_1,x.\_2.sum)) |
| **reduceByKey**(func, [numTasks])                        | 在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置 | sc.textFile("/root/words.txt").flatMap(x=>x.split(" ")).map((\_,1)).reduceByKey(\_+\_).sortBy(\_.\_2,false).collect |
| **aggregateByKey**(zeroValue)(seqOp, combOp, [numTasks]) | aggregate接收两个函数，和一个初始化值。seqOp函数用于聚集每一个分区，combOp用于聚集所有分区聚集后的结果。每一个分区的聚集，和最后所有分区的聚集都需要初始化值的参与。 | val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2); rdd1.aggregate(0)(math.max(\_,\_), \_ + \_) |
| **sortByKey**([ascending], [numTasks])                   | 在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD |                                                              |
| **sortBy**(func,[ascending], [numTasks])                 | 与sortByKey类似，但是更灵活                                  | rdd8.distinct.sortBy(x=>x).collect                           |
| **join**(otherDataset, [numTasks])                       | 在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD | val rdd1 = sc.parallelize(List(("tom", 1), ("jerry", 2), ("kitty", 3))); val rdd2 = sc.parallelize(List(("jerry", 9), ("tom", 8), ("shuke", 7))); val rdd3 = rdd1.join(rdd2) |
| **cogroup**(otherDataset, [numTasks])                    | 在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable\<V>,Iterable\<W>))类型的RDD | val rdd1 = sc.parallelize(List(("tom", 1), ("tom", 2), ("jerry", 3), ("kitty", 2))); val rdd2 = sc.parallelize(List(("jerry", 2), ("tom", 1), ("shuke", 2))); val rdd3 = rdd1.cogroup(rdd2) |
| **cartesian**(otherDataset)                              | 笛卡尔积                                                     | val rdd1 = sc.parallelize(List("tom", "jerry")); val rdd2 = sc.parallelize(List("tom", "kitty", "shuke")); val rdd3 = rdd1.cartesian(rdd2) |
| **pipe**(command, [envVars])                             | 调用外部命令                                                 |                                                              |
| **coalesce**(numPartitions**)**                          | 重新分区                                                     |                                                              |
| **repartition**(numPartitions)                           | 重新分区                                                     |                                                              |
### Action

**常用的Action**

| **动作**                                      | **含义**                                                     |
| --------------------------------------------- | ------------------------------------------------------------ |
| **reduce**(func)                              | 通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的 |
| **collect**()                                 | 在驱动程序中，以数组的形式返回数据集的所有元素               |
| **count**()                                   | 返回RDD的元素个数                                            |
| **first**()                                   | 返回RDD的第一个元素（类似于take(1)）                         |
| **take**(n)                                   | 返回一个由数据集的前n个元素组成的数组                        |
| **takeSample**(withReplacement*,*num, [seed]) | 返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子 |
| **takeOrdered**(n, [ordering])                |                                                              |
| **saveAsTextFile**(path)                      | 将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本 |
| **saveAsSequenceFile**(path)                  | 将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。 |
| **saveAsObjectFile**(path)                    |                                                              |
| **countByKey**()                              | 针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。 |
| **foreach**(func)                             | 在数据集的每一个元素上，运行函数func进行更新。               |

### WordCount 中的RDD

![1585660688319](/img/1585660688319.png)

### 场景训练

**练习1：批量操作后过滤**

val rdd1 = sc.parallelize(List(5, 6, 4, 7, 3, 8, 2, 9, 1, 10))

//对rdd1里的每一个元素乘2然后排序

val rdd2 = rdd1.map(_ * 2).sortBy(x => x, true)

//过滤出大于等于十的元素

val rdd3 = rdd2.filter(_ >= 10)

//将元素以数组的方式在客户端显示

rdd3.collect

**练习2：数据打平**

val rdd1 = sc.parallelize(Array("a b c", "d e f", "h i j"))

//将rdd1里面的每一个元素先切分在压平

val rdd2 = rdd1.flatMap(_.split(' '))

rdd2.collect

**练习3：求两个RDD的交集或并集**

val rdd1 = sc.parallelize(List(5, 6, 4, 3))

val rdd2 = sc.parallelize(List(1, 2, 3, 4))

//求并集

val rdd3 = rdd1.union(rdd2)

//求交集

val rdd4 = rdd1.intersection(rdd2)

//去重

rdd3.distinct.collect

rdd4.collect

**练习4：数据合并分组**

val rdd1 = sc.parallelize(List(("tom", 1), ("jerry", 3), ("kitty", 2)))

val rdd2 = sc.parallelize(List(("jerry", 2), ("tom", 1), ("shuke", 2)))

//求jion

val rdd3 = rdd1.join(rdd2)

rdd3.collect

//求并集

val rdd4 = rdd1 union rdd2

//按key进行分组

rdd4.groupByKey

rdd4.collect

**练习5：数据合并分组**

val rdd1 = sc.parallelize(List(("tom", 1), ("tom", 2), ("jerry", 3), ("kitty", 2)))

val rdd2 = sc.parallelize(List(("jerry", 2), ("tom", 1), ("shuke", 2)))

//cogroup

val rdd3 = rdd1.cogroup(rdd2)

//注意cogroup与groupByKey的区别

rdd3.collect

**练习6：数据聚合**

val rdd1 = sc.parallelize(List(1, 2, 3, 4, 5))

//reduce聚合

val rdd2 = rdd1.reduce(_ + _)

rdd2.collect

**练习7：数据合并聚合并排序**

val rdd1 = sc.parallelize(List(("tom", 1), ("jerry", 3), ("kitty", 2),  ("shuke", 1)))

val rdd2 = sc.parallelize(List(("jerry", 2), ("tom", 3), ("shuke", 2), ("kitty", 5)))

val rdd3 = rdd1.union(rdd2)

//按key进行聚合

val rdd4 = rdd3.reduceByKey(_ + _)

rdd4.collect

//按value的降序排序

val rdd5 = rdd4.map(t => (t._2, t._1)).sortByKey(false).map(t => (t._2, t._1))

rdd5.collect

> 想要了解更多，访问下面的地址
>
> http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html
>

### RDD 的依赖关系

RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。

![1585660782358](/img/1585660782358.png)

#### 窄依赖

窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用

总结：窄依赖我们形象的比喻为**独生子女**

#### 宽依赖

宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition

总结：窄依赖我们形象的比喻为**超生**

#### RDD的血统

RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区

### RDD 的缓存

#### RDD缓存的作用

Spark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或缓存个数据集。当持久化某个RDD后，每一个节点都将把计算的分片结果保存在内存中，并在对此RDD或衍生出的RDD进行的其他动作中重用。这使得后续的动作变得更加迅速。RDD相关的持久化和缓存，是Spark最重要的特征之一。可以说，缓存是Spark构建迭代式算法和快速交互式查询的关键。

#### RDD 缓存的方式

RDD通过persist方法或cache方法可以将前面的计算结果缓存，但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。

![通过查看源码发现cache最终也是调用了persist方法，默认的存储级别都是仅在内存存储一份，Spark的存储级别还有好多种，存储级别在object StorageLevel中定义的。1585660993497](/img/1585660993497.png)

![1585661013707](/img/1585661013707.png)

缓存有可能丢失，或者存储存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。

### DAG的生成

DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。

![1585661122634](/img/1585661122634.png)

### Spark 中的checkpoint

checkpoint在spark中主要有两块应用：一块是在spark core中对RDD做checkpoint，可以将checkpoint RDD的依赖关系，RDD数据保存到可靠存储（如HDFS）以便数据恢复；另外一块是应用在spark streaming中，使用checkpoint用来保存DStreamGraph以及相关配置信息，以便在Driver崩溃重启的时候能够接着之前进度继续进行处理（如之前waiting batch的job会在重启后继续处理）。

本文主要将详细分析checkpoint在以上两种场景的读写过程。

#### checkpoint的使用方法

使用checkpoint对RDD做快照大体如下：

> ```scala
> sc.setCheckpointDir(checkpointDir.toString)
> val rdd = sc.makeRDD(1 to 20, numSlices = 1)
> rdd.checkpoint()
> ```

首先，设置checkpoint的目录（一般是hdfs目录），这个目录用来将RDD相关的数据（包括每个partition实际数据，以及partitioner（如果有的话））。然后在RDD上调用checkpoint的方法即可。

#### checkpoint写流程

可以看到checkpoint使用非常简单，设置checkpoint目录，然后调用RDD的checkpoint方法。针对checkpoint的写入流程，主要有以下四个问题：

Q1：RDD中的数据是什么时候写入的？是在rdd调用checkpoint方法时候吗？

Q2：在做checkpoint的时候，具体写入了哪些数据到HDFS了？

Q3：在对RDD做完checkpoint以后，对做RDD的本省又做了哪些收尾工作？

Q4：实际过程中，使用RDD做checkpoint的时候需要注意什么问题？

弄清楚了以上四个问题，我想对checkpoint的写过程也就基本清楚了。接下来将一一回答上面提出的问题。

A1：首先看一下RDD中checkpoint方法，可以看到在该方法中是只是新建了一个ReliableRDDCheckpintData的对象，并没有做实际的写入工作。实际触发写入的时机是在runJob生成改RDD后，调用RDD的doCheckpoint方法来做的。

A2：在经历调用RDD.doCheckpoint → RDDCheckpintData.checkpoint → ReliableRDDCheckpintData.doCheckpoint → ReliableRDDCheckpintData.writeRDDToCheckpointDirectory后，在writeRDDToCheckpointDirectory方法中可以看到：将作为一个单独的任务（RunJob）将RDD中每个parition的数据依次写入到checkpoint目录（writePartitionToCheckpointFile），此外如果该RDD中的partitioner如果不为空，则也会将该对象序列化后存储到checkpoint目录。所以，在做checkpoint的时候，写入的hdfs中的数据主要包括：RDD中每个parition的实际数据，以及可能的partitioner对象（writePartitionerToCheckpointDir）。

A3：在写完checkpoint数据到hdfs以后，将会调用rdd的markCheckpoined方法，主要斩断该rdd的对上游的依赖，以及将paritions置空等操作。

A4：通过A1，A2可以知道，在RDD计算完毕后，会再次通过RunJob将每个partition数据保存到HDFS。这样RDD将会计算两次，所以为了避免此类情况，最好将RDD进行cache。即1.1中rdd的推荐使用方法如下：

> ```
> sc.setCheckpointDir(checkpointDir.toString)
> val rdd = sc.makeRDD(1 to 20, numSlices = 1)
> rdd.cache()
> rdd.checkpoint()
> ```

#### checkpoint 读流程

在做完checkpoint后，获取原来RDD的依赖以及partitions数据都将从CheckpointRDD中获取。也就是说获取原来rdd中每个partition数据以及partitioner等对象，都将转移到CheckPointRDD中。

在CheckPointRDD的一个具体实现ReliableRDDCheckpintRDD中的compute方法中可以看到，将会从hdfs的checkpoint目录中恢复之前写入的partition数据。而partitioner对象（如果有）也会从之前写入hdfs的paritioner对象恢复。

总的来说，checkpoint读取过程是比较简单的。

#### checkpoint 和cache 区别

- cache 机制保证了需要访问重复数据的应用（如迭代型算法和交互式应用）可以运行的更快。
- 与 Hadoop MapReduce job 不同的是 Spark 的逻辑/物理执行图可能很庞大，task 中 computing chain 可能会很长，计算某些 RDD 也可能会很耗时。这时，如果 task 中途运行出错，那么 task 的整个 computing chain 需要重算，代价太高。因此，有必要将计算代价较大的 RDD checkpoint 一下，这样，当下游 RDD 计算出错时，可以直接从 checkpoint 过的 RDD 那里读取数据继续算。

### RDD分区规则

1. **通过集合方式指定**

   通过scala 集合方式parallelize生成rdd，

   如， val rdd = sc.parallelize(1 to 10)

   这种方式下，如果在parallelize操作时没有指定分区数，则

   **rdd的分区数 = sc.defaultParallelism**

2. **textFile 分区规则**

   1.如果textFile指定分区数量为0或者1的话，defaultMinPartitions值为1，则有多少个文件，就会有多少个分区。

   2.如果不指定默认分区数量，则默认分区数量为2，则会根据所有文件字节大小totalSize除以分区数量partitons的值goalSize，然后比较goalSize和hdfs指定分块大小（这里是32M）作比较，以较小的最为goalSize作为切分大小，对每个文件进行切分，若文件大于大于goalSize，则会生成该文件大小/goalSize + 1个分区。

   3.如果指定分区数量大于等于2，则默认分区数量为指定值，生成分区数量规则同2中的规则。

## Dataframe(SQL)

### 什么是Dataframe

与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上 看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。由于与R和Pandas的DataFrame类似，Spark DataFrame很好地继承了传统单机数据分析的开发体验。

### 创建Dataframe

1. 在本地创建一个文件，有三列，分别是id、name、age，用空格分隔，然后上传到hdfs上

   hdfs dfs -put person.txt /

2. 在spark shell执行下面命令，读取数据，将每一行的数据使用列分隔符分割

   val lineRDD = sc.textFile("hdfs://node1.itcast.cn:9000/person.txt").map(_.split(" "))

3. 定义case class（相当于表的schema）

   case class Person(id:Int, name:String, age:Int)

4. 将RDD和case class关联

   val personRDD = lineRDD.map(x => Person(x(0).toInt, x(1), x(2).toInt))

5. 将RDD转换成DataFrame

   val personDF = personRDD.toDF

6. 对DataFrame进行处理

   personDF.show

   ![1588066486223](/../img/1588066486223.png)

### Dataframe 常用操作

**查看Dataframe 中内容**

personDF.show

**查看DataFrame部分列中的内容**

personDF.select(personDF.col("name")).show

personDF.select(col("name"), col("age")).show

personDF.select("name").show

**打印dataFrame的Schema信息**

personDF.printSchema

**查看内容，并对字段进行计算**

personDF.select(col("id"), col("name"), col("age") + 1).show

personDF.select(personDF("id"), personDF("name"), personDF("age") + 1).show

**字段条件过滤**

personDF.filter(col("age") >= 18).show

**分组计算**

personDF.groupBy("age").count().show()

### SQL语法风格

**将Dataframe注册成表**

personDF.registerTempTable("t_person")

**sql排序查询**

sqlContext.sql("select * from t_person order by age desc limit 2").show

**查询schema信息**

sqlContext.sql("desc t_person").show

## Spark Streaming

### SparkStreaming 的特殊Transformation

1. **UpdateStateByKey Operation**

   UpdateStateByKey原语用于记录历史记录，上文中Word Count示例中就用到了该特性。若不用UpdateStateByKey来更新状态，那么每次数据进来后分析完成后，结果输出后将不在保存

2. **Transform Operation**

   Transform原语允许DStream上执行任意的RDD-to-RDD函数。通过该函数可以方便的扩展Spark API。此外，MLlib（机器学习）以及Graphx也是通过本函数来进行结合的。

3. **Window Operations**

   Window Operations有点类似于Storm中的State，可以设置窗口的大小和滑动窗口的间隔来动态的获取当前Steaming的允许状态

![1588069052538](/../img/1588069052538.png)

### Output Operations on DStreams

Output Operations可以将DStream的数据输出到外部的数据库或文件系统，当某个Output Operations原语被调用时（与RDD的Action相同），streaming程序才会开始真正的计算过程。

| Output Operation                    | Meaning                                                      |
| ----------------------------------- | ------------------------------------------------------------ |
| print()                             | Prints the first ten elements of every batch of data in a DStream on the driver node running the streaming application. This is useful for development and debugging. |
| saveAsTextFiles(prefix, [suffix])   | Save this DStream's contents as text files. The file name at each batch interval is generated based on prefix and suffix: "prefix-TIME_IN_MS[.suffix]". |
| saveAsObjectFiles(prefix, [suffix]) | Save this DStream's contents as SequenceFiles of serialized Java objects. The file name at each batch interval is generated based on prefix and suffix: "prefix-TIME_IN_MS[.suffix]". |
| saveAsHadoopFiles(prefix, [suffix]) | Save this DStream's contents as Hadoop files. The file name at each batch interval is generated based on prefix and suffix: "prefix-TIME_IN_MS[.suffix]". |
| foreachRDD(func)                    | The most generic output operator that applies a function, func, to each RDD generated from the stream. This function should push the data in each RDD to an external system, such as saving the RDD to files, or writing it over the network to a database. Note that the function func is executed in the driver process running the streaming application, and will usually have RDD actions in it that will force the computation of the streaming RDDs. |

## Spark 任务提交

### 任务提交的主要四个阶段

DAG的生成 => stage切分 => task的生成 => 任务提交

![1585970735319](/img/1585970735319.png)

1. 构建DAG
   用户提交的job将首先被转换成一系列RDD并通过RDD之间的依赖关系构建DAG,然后将DAG提交到调度系统；
2. DAGScheduler将DAG切分stage（切分依据是shuffle）,将stage中生成的task以taskset的形式发送给TaskScheduler
3. Scheduler 调度task（根据资源情况将task调度到Executors）
4. Executors接收task，然后将task交给线程池执行。

### 任务提交详细步骤

1. spark集群启动后，Worker向Master注册信息

![img](https://www.linuxidc.com/upload/2018_02/180211173461711.png)

1. spark-submit命令提交程序后，driver和application也会向Master注册信息

![img](https://www.linuxidc.com/upload/2018_02/180211173461712.png)

![img](https://www.linuxidc.com/upload/2018_02/180211173461713.png)

1. 创建SparkContext对象：主要的对象包含DAGScheduler和TaskScheduler
2. Driver把Application信息注册给Master后，Master会根据App信息去Worker节点启动Executor
3. Executor内部会创建运行task的线程池，然后把启动的Executor反向注册给Dirver
4. DAGScheduler：负责把Spark作业转换成Stage的DAG（Directed Acyclic Graph有向无环图），根据宽窄依赖切分Stage，然后把Stage封装成TaskSet的形式发送个TaskScheduler；同时DAGScheduler还会处理由于Shuffle数据丢失导致的失败；
5. TaskScheduler：维护所有TaskSet，分发Task给各个节点的Executor（根据数据本地化策略分发Task），监控task的运行状态，负责重试失败的task；
6. 所有task运行完成后，SparkContext向Master注销，释放资源；

### Spark stage 切分流程

![1585970495777](/img/1585970495777.png)

#### 划分stage 的思路

park划分stage的整体思路是：从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中。

#### stage 作用

这是个复杂是业务逻辑（将多台机器上具有相同属性的数据聚合到一台机器上:shuffle）如果有shuffle，那么就意味着前面阶段产生结果后，才能执行下一个阶段，下一个阶段的计算依赖上一个阶段的数据在同一个stage中，会有多个算子，可以合并到一起，我们很难‘’称其为pipeline（流水线，严格按照流程、顺序执行）

### Spark Driver  给Executor 提交task 时序图

![1585970924876](/img/1585970924876.png)

### Spark Executor 启动和任务接受和执行时序图

![1585970969792](/img/1585970969792.png)

## Spark Shuffle

### 概述

Shuffle就是对数据进行重组，由于分布式计算的特性和要求，在实现细节上更加繁琐和复杂

在MapReduce框架，Shuffle是连接Map和Reduce之间的桥梁，Map阶段通过shuffle读取数据并输出到对应的Reduce；而Reduce阶段负责从Map端拉取数据并进行计算。在整个shuffle过程中，往往伴随着大量的磁盘和网络I/O。所以shuffle性能的高低也直接决定了整个程序的性能高低。Spark也会有自己的shuffle实现过程

![1586768403401](/img/1586768403401.png)

在DAG调度的过程中，Stage阶段的划分是根据是否有shuffle过程，也就是存在ShuffleDependency宽依赖的时候，需要进行shuffle,这时候会将作业job划分成多个Stage；并且在划分Stage的时候，构建ShuffleDependency的时候进行shuffle注册，获取后续数据读取所需要的ShuffleHandle,最终每一个job提交后都会生成一个ResultStage和若干个ShuffleMapStage，其中ResultStage表示生成作业的最终结果所在的Stage. ResultStage与ShuffleMapStage中的task分别对应着ResultTask与ShuffleMapTask。一个作业，除了最终的ResultStage外，其他若干ShuffleMapStage中各个ShuffleMapTask都需要将最终的数据根据相应的Partitioner对数据进行分组，然后持久化分区的数据。

 ![1586768416749](/img/1586768416749.png)

### HashShuffle机制

#### HashShuffle概述

在spark-1.6版本之前，一直使用HashShuffle，在spark-1.6版本之后使用Sort-Base Shuffle，因为HashShuffle存在的不足所以就替换了HashShuffle.

我们知道，Spark的运行主要分为2部分：一部分是驱动程序，其核心是SparkContext；另一部分是Worker节点上Task,它是运行实际任务的。程序运行的时候，Driver和Executor进程相互交互：运行什么任务，即Driver会分配Task到Executor，Driver 跟 Executor 进行网络传输; 任务数据从哪儿获取，即Task要从 Driver 抓取其他上游的 Task 的数据结果，所以有这个过程中就不断的产生网络结果。其中，下一个 Stage 向上一个 Stage 要数据这个过程，我们就称之为 Shuffle。

#### 没有优化之前的HashShuffle机制

![1586768469257](/img/1586768469257.png)

1. 在HashShuffle没有优化之前，每一个ShufflleMapTask会为每一个ReduceTask创建一个bucket缓存，并且会为每一个bucket创建一个文件。这个bucket存放的数据就是经过Partitioner操作(默认是HashPartitioner)之后找到对应的bucket然后放进去，最后将数据刷新bucket缓存的数据到磁盘上，即对应的block file.
2. 然后ShuffleMapTask将输出作为MapStatus发送到DAGScheduler的MapOutputTrackerMaster，每一个MapStatus包含了每一个ResultTask要拉取的数据的位置和大小
3. ResultTask然后去利用BlockStoreShuffleFetcher向MapOutputTrackerMaster获取MapStatus，看哪一份数据是属于自己的，然后底层通过BlockManager将数据拉取过来
4. 拉取过来的数据会组成一个内部的ShuffleRDD，优先放入内存，内存不够用则放入磁盘，然后ResulTask开始进行聚合，最后生成我们希望获取的那个MapPartitionRDD

**这种方式的缺点**

如上图所示：在这里有1个worker，2个executor，每一个executor运行2个ShuffleMapTask，有三个ReduceTask，所以总共就有4 * 3=12个bucket和12个block file。

如果数据量较大，将会生成M*R个小文件，比如ShuffleMapTask有100个，ResultTask有100个，这就会产生100*100=10000个小文件

bucket缓存很重要，需要将ShuffleMapTask所有数据都写入bucket，才会刷到磁盘，那么如果Map端数据过多，这就很容易造成内存溢出，尽管后面有优化，bucket写入的数据达到刷新到磁盘的阀值之后，就会将数据一点一点的刷新到磁盘，但是这样磁盘I/O就多了

#### 优化后的HashShuffle

![1586768527736](/img/1586768527736.png)

1. 每一个Executor进程根据核数，决定Task的并发数量，比如executor核数是2，就是可以并发运行两个task，如果是一个则只能运行一个task

2. 假设executor核数是1，ShuffleMapTask数量是M,那么它依然会根据ResultTask的数量R，创建R个bucket缓存，然后对key进行hash，数据进入不同的bucket中，每一个bucket对应着一个block file,用于刷新bucket缓存里的数据

3. 然后下一个task运行的时候，那么不会再创建新的bucket和block file，而是复用之前的task已经创建好的bucket和block file。即所谓同一个Executor进程里所有Task都会把相同的key放入相同的bucket缓冲区中

4. 这样的话，生成文件的数量就是(本地worker的executor数量\*executor的cores\*ResultTask数量)如上图所示，即2 \* 1\* 3 = 6个文件，每一个Executor的shuffleMapTask数量100,ReduceTask数量为100，那么

   > 未优化的HashShuffle的文件数是2 \*1\* 100\*100 =20000，优化之后的数量是2\*1\*100 = 200文件，相当于少了100倍

**这种方式的缺点**：

如果 Reducer 端的并行任务或者是数据分片过多的话则 Core * Reducer Task 依旧过大，也会产生很多小文件。

### Sort-Based Shuffle

#### Sort-Based Shuffle概述

**HashShuffle回顾**

1. io\GC\内存占用大:  HashShuffle写数据的时候，内存有一个bucket缓冲区，同时在本地磁盘有对应的本地文件，如果本地有文件，那么在内存应该也有文件句柄也是需要耗费内存的。也就是说，从内存的角度考虑，即有一部分存储数据，一部分管理文件句柄。如果Mapper分片数量为1000,Reduce分片数量为1000,那么总共就需要1000000个小文件。所以就会有很多内存消耗，频繁IO以及GC频繁或者出现内存溢出。
2. 容易造成网络异常:   而且Reducer端读取Map端数据时，Mapper有这么多小文件，就需要打开很多网络通道读取，很容易造成Reducer（下一个stage）通过driver去拉取上一个stage数据的时候，说文件找不到，其实不是文件找不到而是程序不响应，因为正在GC.

#### Sorted-Based Shuffle介绍

为了缓解Shuffle过程产生文件数过多和Writer缓存开销过大的问题，spark引入了类似于hadoop Map-Reduce的shuffle机制。该机制每一个ShuffleMapTask不会为后续的任务创建单独的文件，而是会将所有的Task结果写入同一个文件，并且对应生成一个索引文件。以前的数据是放在内存缓存中，等到数据完了再刷到磁盘，现在为了减少内存的使用，在内存不够用的时候，可以将输出溢写到磁盘，结束的时候，再将这些不同的文件联合内存的数据一起进行归并，从而减少内存的使用量。一方面文件数量显著减少，另一方面减少Writer缓存所占用的内存大小，而且同时避免GC的风险和频率。

![1586768569212](/img/1586768569212.png)

Sort-Based Shuffle有几种不同的策略：**BypassMergeSortShuffleWriter、SortShuffleWriter和UnasfeSortShuffleWriter。**

**对于BypassMergeSortShuffleWriter**

使用这个模式特点：

1. 主要用于处理不需要排序和聚合的Shuffle操作，所以数据是直接写入文件，数据量较大的时候，网络I/O和内存负担较重
2. 主要适合处理Reducer任务数量比较少的情况下
3. 将每一个分区写入一个单独的文件，最后将这些文件合并,减少文件数量；但是这种方式需要并发打开多个文件，对内存消耗比较大
4. 因为BypassMergeSortShuffleWriter这种方式比SortShuffleWriter更快，所以如果在Reducer数量不大，又不需要在map端聚合和排序，而且最终产生的文件数量少，尽量使用这种方式进行shuffle。 
5. Reducer的数目 <  spark.shuffle.sort.bypassMergeThrshold指定的阀值，就是用的是这种方式。

**对于SortShuffleWriter**

使用这个模式特点：

1. 比较适合数据量很大的场景或者集群规模很大
2. 引入了外部外部排序器，可以支持在Map端进行本地聚合或者不聚合
3. 如果外部排序器enable了spill功能，如果内存不够，可以先将输出溢写到本地磁盘，最后将内存结果和本地磁盘的溢写文件进行合并

> 另外这个Sort-Based Shuffle跟Executor核数没有关系，即跟并发度没有关系，它是每一个ShuffleMapTask都会产生一个data文件和index文件，所谓合并也只是将该ShuffleMapTask的各个partition对应的分区文件合并到data文件而已。所以这个就需要个Hash-BasedShuffle的consolidation机制区别开来。